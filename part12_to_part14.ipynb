{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ[\"LANGCHAIN_API\"] \n",
    "\n",
    "gemini_api = os.environ[\"GEMINI_API\"]\n",
    "\n",
    "llm = GoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "                       header_template= headers)\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\",\n",
    "                       header_template= headers)\n",
    "docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid \n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x : x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document: \\n \\n {doc}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "\n",
    ")\n",
    "summaries = chain.batch(docs, {\"max_concurrency\" : 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.storage import InMemoryByteStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_classic.retrievers import MultiVectorRetriever\n",
    "\n",
    "vectorstore = Chroma(collection_name = \"summaries\",\n",
    "                     embedding_function = embeddings)\n",
    "\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore = vectorstore,\n",
    "    byte_store= store,\n",
    "    id_key = id_key\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "summary_docs = [\n",
    "    Document(page_content = s, metadata = {id_key : doc_ids[i]})\n",
    "    for i , s in enumerate(summaries)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Memory in agents\"\n",
    "sub_docs = vectorstore.similarity_search(query,k=1)\n",
    "sub_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(query,n_results=1)\n",
    "retrieved_docs[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part - 13 raptor - see youtube video on Langchain official channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part - 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_wikipedia_page(title: str):\n",
    "    \"\"\"\n",
    "    Retrieve the full text content of a Wikipedia page.\n",
    "\n",
    "    :param title: str - Title of the Wikipedia page.\n",
    "    :return: str - Full text content of the page as raw string.\n",
    "    \"\"\"\n",
    "    # Wikipedia API endpoint\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "    }\n",
    "\n",
    "    # Custom User-Agent header to comply with Wikipedia's best practices\n",
    "    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1 (ben@clavie.eu)\"}\n",
    "\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extracting page content\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return page[\"extract\"] if \"extract\" in page else None\n",
    "\n",
    "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG.index(\n",
    "    collection=[full_document],\n",
    "    index_name=\"Miyazaki-123\",\n",
    "    max_document_length=180,\n",
    "    split_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = RAG.search(query=\"What animation studio did Miyazaki found?\", k=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RAG.as_langchain_retriever(k=3)\n",
    "retriever.invoke(\"What animation studio did Miyazaki found?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
