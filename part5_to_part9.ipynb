{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.environ[\"LANGCHAIN_API\"] \n",
    "\n",
    "gemini_api = os.environ[\"GEMINI_API\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part - 5 Multi Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "#load blog\n",
    " \n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths = (\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs= dict(\n",
    "        parse_only = bs4.SoupStrainer(\n",
    "            class_ = (\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)\n",
    "\n",
    "splits= text_splitter.split_documents(blog_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index \n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-001\", \n",
    "    api_key=gemini_api,\n",
    "    credentials= None\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents = splits,\n",
    "                                   embedding = embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an AI language model assistant, Your takse is to generate five different versions of the given user questions and to retrieve relevant documents from the database. By generating multiple different perspectives on the user question, your goal is to make the user overcome some of the limitations of the distance based similarity search, provide these alternatives responses separated by newlines. Original question {question}\"\"\"\n",
    "\n",
    "prompt_pers = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_pers \n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumps, loads\n",
    "\n",
    "def get_uniq(documents : list[list]):\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "\n",
    "    return [loads(doc) for doc in unique_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhruv\\AppData\\Local\\Temp\\ipykernel_12592\\4290818311.py:7: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieve\n",
    "\n",
    "question = \"what is task decomposition for LLM agents ?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_uniq\n",
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "#RAG \n",
    "\n",
    "template = \"\"\"answer the following question based on this context:\n",
    "        {context}\n",
    "        \n",
    "        Question : {question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down large, complex tasks into smaller, more manageable subgoals. This process allows the agent to handle intricate tasks more efficiently. This decomposition can be achieved through several methods:\\n\\n*   **LLM prompting:** Using simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\"\\n*   **Task-specific instructions:** Providing explicit instructions tailored to the task, such as \"Write a story outline\" for novel writing.\\n*   **Human inputs:** Incorporating guidance from human users.\\n*   **Chain of Thought (CoT):** Instructing the model to \"think step by step\" to decompose hard tasks into simpler ones.\\n*   **Tree of Thoughts (ToT):** Expanding on CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thoughts.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rag_chain.invoke({\"question\" : question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 : RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI language model assistant, Your task is to generate five different versions of the given user questions and to retrieve relevant documents from the database. By generating multiple different perspectives on the user question, your goal is to make the user overcome some of the limitations of the distance based similarity search, provide these alternatives responses separated by newlines. Original question {question}\"\"\"\n",
    "\n",
    "prompt_pers = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k = 60):\n",
    "    \"\"\"reciprocal rank fusion function that takes in multiple lists from ranked documents and an optional paramater k is used in the RRF formula\"\"\"\n",
    "\n",
    "    fused_scores = {}\n",
    "\n",
    "    for rank, doc in enumerate(docs):\n",
    "        doc_str = dumps(doc)\n",
    "\n",
    "        if doc_str not in fused_scores:\n",
    "            fused_scores[doc_str] = 0\n",
    "        \n",
    "        prev_score = fused_scores[doc_str]\n",
    "        fused_scores[doc_str] += 1 / (rank + k)\n",
    "    \n",
    "    reranked_res = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key = lambda x: x[1], reverse = True)\n",
    "    ]\n",
    "\n",
    "    return reranked_res\n",
    "\n",
    "ret_chain_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = ret_chain_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Task decomposition for LLM agents refers to the process of breaking down large, complex tasks into smaller, more manageable subgoals. This enables the agent to handle intricate tasks more efficiently. This decomposition can be achieved through various methods, including:',\n",
       " '',\n",
       " '*   **LLM prompting:** Using simple instructions like \"Steps for XYZ.\\\\\\\\n1.\" or \"What are the subgoals for achieving XYZ?\".',\n",
       " '*   **Task-specific instructions:** Providing explicit directives for particular tasks, such as \"Write a story outline.\" for novel writing.',\n",
       " '*   **Human inputs:** Receiving direct guidance from a human user.',\n",
       " '*   **Chain of Thought (CoT):** Instructing the model to \"think step by step\" to break down difficult tasks into simpler ones.',\n",
       " '*   **Tree of Thoughts (ToT):** Exploring multiple reasoning possibilities at each step, generating several thoughts per step to create a tree structure.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": ret_chain_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
